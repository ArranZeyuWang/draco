{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Bugs\n",
    "\n",
    "Use this notebook to find wrong predicitons. We can then later visualize the pairs of specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draco.learn import data_util\n",
    "from draco.learn import linear\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev, _ = data_util.load_data()\n",
    "\n",
    "X, y = linear.prepare_paired_data(train_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross validate the model by running it over various subsets of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  0.9919354838709677\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Dev score:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "size = len(train_dev)\n",
    "\n",
    "idx = np.arange(size)\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train, test in kf.split(idx):\n",
    "    idx = np.ones(size, dtype=bool)\n",
    "    idx[test] = False\n",
    "    \n",
    "    idx = np.concatenate([idx, idx])\n",
    "    \n",
    "    X_train, X_dev, y_train, y_dev = X[idx], X[~idx], y[idx], y[~idx]\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(solver='sag')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Train score: \", clf.score(X_train, y_train))\n",
    "    print(\"Dev score: \", clf.score(X_dev, y_dev))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  1.0\n",
      "Dev score:  0.9973045822102425\n"
     ]
    }
   ],
   "source": [
    "# For now only run the model once. Later get bugs from every fold of the crossvalidation.\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = data_util.paired_train_test_split(X, y)\n",
    "\n",
    "clf = linear_model.LogisticRegression(solver='sag')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train score: \", clf.score(X_train, y_train))\n",
    "print(\"Dev score: \", clf.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([481])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the pairs that are predicted wrong\n",
    "negative_pairs = X[y == 0]\n",
    "\n",
    "predicted = clf.predict(negative_pairs)\n",
    "bug_idx = predicted > 0.5  # idx in pairs\n",
    "\n",
    "np.arange(len(bug_idx))[bug_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([481])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check, these should be the same as the bugs for positive pairs unless the weight for one feature is 0\n",
    "positive_pairs = X[y == 1]\n",
    "\n",
    "predicted = clf.predict(positive_pairs)\n",
    "bug_idx = predicted < 0.5\n",
    "\n",
    "np.arange(len(bug_idx))[bug_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1], dtype='int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the indexes in the original data\n",
    "bugs = train_dev.index[bug_idx]\n",
    "bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_data = data_util.load_neg_pos_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate better data\n",
    "\n",
    "vals = [{\n",
    "    'q1': random.normalvariate(2, 2),\n",
    "    'q2': random.normalvariate(2, 2),\n",
    "    'n': random.randint(0,7)\n",
    "} for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_specs = []\n",
    "\n",
    "for i in bugs:\n",
    "    example = pos_neg_data[i]\n",
    "    \n",
    "    negative = example.negative\n",
    "    positive = example.positive\n",
    "    \n",
    "    negative['data'] = {\n",
    "        'values': vals\n",
    "    }\n",
    "    positive['data'] = {\n",
    "        'values': vals\n",
    "    }\n",
    "    bug_specs.append({\n",
    "        'true_negative': negative,\n",
    "        'true_positive': positive\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/bugs/bugs.json', 'w') as f:\n",
    "    json.dump(bug_specs, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
