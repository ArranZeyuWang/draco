{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Bugs\n",
    "\n",
    "Use this notebook to find wrong predicitons. We can then later visualize the pairs of specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draco.learn import data_util\n",
    "from draco.learn import linear\n",
    "from sklearn import svm\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev, _ = data_util.load_data()\n",
    "\n",
    "X = train_dev.positive - train_dev.negative\n",
    "X = X.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross validate the model by running it over various subsets of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  0.9919354838709677\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  0.9919354838709677\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n",
      "Train score:  1.0\n",
      "Test score:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    \n",
    "    y_train = np.ones(len(X_train))\n",
    "    \n",
    "    # swap first example\n",
    "    X_train[0] = -X_train[0]\n",
    "    y_train[0] = -y_train[0]\n",
    "\n",
    "    clf = svm.LinearSVC(C=1, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Train score: \", clf.score(X_train, y_train))\n",
    "    print(\"Test score: \", clf.score(X_test, np.ones(len(X_test))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now only run the model once. Later get bugs from every fold of the crossvalidation.\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = data_util.paired_train_test_split(X, y)\n",
    "\n",
    "clf = linear_model.LogisticRegression(solver='sag')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train score: \", clf.score(X_train, y_train))\n",
    "print(\"Dev score: \", clf.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pairs that are predicted wrong\n",
    "negative_pairs = X[y == 0]\n",
    "\n",
    "predicted = clf.predict(negative_pairs)\n",
    "bug_idx = predicted > 0.5  # idx in pairs\n",
    "\n",
    "np.arange(len(bug_idx))[bug_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, these should be the same as the bugs for positive pairs unless the weight for one feature is 0\n",
    "positive_pairs = X[y == 1]\n",
    "\n",
    "predicted = clf.predict(positive_pairs)\n",
    "bug_idx = predicted < 0.5\n",
    "\n",
    "np.arange(len(bug_idx))[bug_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indexes in the original data\n",
    "bugs = train_dev.index[bug_idx]\n",
    "bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_data = data_util.load_neg_pos_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate better data\n",
    "\n",
    "vals = [{\n",
    "    'q1': random.normalvariate(2, 2),\n",
    "    'q2': random.normalvariate(2, 2),\n",
    "    'n': random.randint(0,7)\n",
    "} for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_specs = []\n",
    "\n",
    "for i in bugs:\n",
    "    example = pos_neg_data[i]\n",
    "    \n",
    "    negative = example.negative\n",
    "    positive = example.positive\n",
    "    \n",
    "    negative['data'] = {\n",
    "        'values': vals\n",
    "    }\n",
    "    positive['data'] = {\n",
    "        'values': vals\n",
    "    }\n",
    "    bug_specs.append({\n",
    "        'true_negative': negative,\n",
    "        'true_positive': positive\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/bugs/bugs.json', 'w') as f:\n",
    "    json.dump(bug_specs, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
